"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7152],{3753:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"tutorial/tool-call","title":"Calling external tools","description":"Tool calling is one of the truly \\"LLM native\\" interaction modes that has never existed before.","source":"@site/docs/tutorial/tool-call.md","sourceDirName":"tutorial","slug":"/tutorial/tool-call","permalink":"/tutorial/tool-call","draft":false,"unlisted":false,"editUrl":"https://github.com/GaiaNet-AI/docs/edit/main/docs/tutorial/tool-call.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Gaia nodes with long-term knowledge","permalink":"/tutorial/concepts"},"next":{"title":"Agentic translation on Gaia","permalink":"/tutorial/translator-agent"}}');var o=n(4848),i=n(8453);const s={sidebar_position:1},l="Calling external tools",r={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Run the demo agent",id:"run-the-demo-agent",level:2},{value:"Use the agent",id:"use-the-agent",level:2},{value:"Make it robust",id:"make-it-robust",level:2}];function c(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"calling-external-tools",children:"Calling external tools"})}),"\n",(0,o.jsx)(t.p,{children:'Tool calling is one of the truly "LLM native" interaction modes that has never existed before.\nIt gives the "thinking" LLMs the ability to "act" -- both in acquiring new knowledge and in performing real world actions. It is a crucial part of any agentic application.'}),"\n",(0,o.jsx)(t.p,{children:"Open source LLMs are increasingly good at using tools. The Llama 3 models have now made it possible to have reliable tool calling performance on 8b class of LLMs running on your own laptop!"}),"\n",(0,o.jsx)(t.p,{children:"In this tutorial, we will show you a simple Python program that allows a local LLM to run code and manipulate data on the local computer!"}),"\n",(0,o.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(t.p,{children:"You will need a Gaia node ready to provide LLM services through a public URL. You can"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.a,{href:"../getting-started/quick-start",children:"run your own node"}),". You will need to start a Gaia node for the ",(0,o.jsx)(t.a,{href:"https://github.com/GaiaNet-AI/node-configs/tree/main/llama-3.3-70b-instruct",children:"Llama 3.3 70B model"})," or the ",(0,o.jsx)(t.a,{href:"https://github.com/GaiaNet-AI/node-configs/tree/main/llama-3.1-8b-instruct",children:"Llama 3.1 8B model"})," or the ",(0,o.jsx)(t.a,{href:"https://github.com/GaiaNet-AI/node-configs/tree/main/llama-3-groq-8b-tool",children:"Llama 3 Groq 8B model"}),". You can then use the node's API URL endpoint and model name in your tool call apps."]}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"../nodes",children:"use a public node"})}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"In this tutorial, we will use a public Llama 3.3 node with the function call support."}),"\n",(0,o.jsxs)(t.table,{children:[(0,o.jsx)(t.thead,{children:(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.th,{children:"Attribute"}),(0,o.jsx)(t.th,{children:"Value"})]})}),(0,o.jsxs)(t.tbody,{children:[(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"API endpoint URL"}),(0,o.jsx)(t.td,{children:(0,o.jsx)(t.a,{href:"https://llama70b.gaia.domains/v1",children:"https://llama70b.gaia.domains/v1"})})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"Model Name"}),(0,o.jsx)(t.td,{children:"llama70b"})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"API KEY"}),(0,o.jsx)(t.td,{children:"gaia"})]})]})]}),"\n",(0,o.jsx)(t.h2,{id:"run-the-demo-agent",children:"Run the demo agent"}),"\n",(0,o.jsxs)(t.p,{children:["The ",(0,o.jsx)(t.a,{href:"https://github.com/second-state/llm_todo",children:"agent app"})," is written in Python. It demonstrates how the LLM could use tools to operate a SQL database. In this case, it starts and operates an in-memory SQLite database. The database stores a list of todo items."]}),"\n",(0,o.jsx)(t.p,{children:"Download the code and install the Python dependencies as follows."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"git clone https://github.com/second-state/llm_todo\ncd llm_todo\npip install -r requirements.txt\n"})}),"\n",(0,o.jsx)(t.p,{children:"Set the environment variables for the API server and model name we just set up."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'export OPENAI_MODEL_NAME="llama"\nexport OPENAI_BASE_URL= "https://llama8b.gaia.domains/v1"\n'})}),"\n",(0,o.jsxs)(t.p,{children:["Run the ",(0,o.jsx)(t.code,{children:"main.py"})," application and bring up the command line chat interface."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"python main.py\n"})}),"\n",(0,o.jsx)(t.h2,{id:"use-the-agent",children:"Use the agent"}),"\n",(0,o.jsx)(t.p,{children:"Now, you can ask the LLM to perform tasks. For example, you can say"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"User: \nHelp me to write down it I'm going to have a meeting with the marketing team.\n"})}),"\n",(0,o.jsx)(t.p,{children:"The LLM understands that you need to insert a record into the database and returns a tool call response in JSON."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'Assistant:\n<tool_call>\n{"id": 0, "name": "create_task", "arguments": {"task": "have a meeting with the marketing team"}}\n</tool_call>\n'})}),"\n",(0,o.jsxs)(t.p,{children:["The agent app (i.e., ",(0,o.jsx)(t.code,{children:"main.py"}),") executes the tool call ",(0,o.jsx)(t.code,{children:"create_task"})," in the JSON response, and sends back the results as role ",(0,o.jsx)(t.code,{children:"Tool"}),". You do not need to do anything here as it happens automatically in ",(0,o.jsx)(t.code,{children:"main.py"}),". The SQLite database is updated when the agent app executes the tool call."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"Tool:\n[{'result': 'ok'}]\n"})}),"\n",(0,o.jsx)(t.p,{children:"The LLM receives the execution result and then answers you."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"Assistant:\nI've added \"have a meeting with the marketing team\" to your task list. Is there anything else you'd like to do?\n"})}),"\n",(0,o.jsx)(t.p,{children:"You can continue the conversation."}),"\n",(0,o.jsxs)(t.p,{children:["To learn more about how tool calling works, see ",(0,o.jsx)(t.a,{href:"https://github.com/LlamaEdge/LlamaEdge/blob/main/api-server/ToolUse.md",children:"this article"}),"."]}),"\n",(0,o.jsx)(t.h2,{id:"make-it-robust",children:"Make it robust"}),"\n",(0,o.jsx)(t.p,{children:"One of the major challenges for LLM applications is the frequent unreliability of their responses. For example:"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.em,{children:"If the LLM generates an incorrect tool call that fails to address the user\u2019s query,"})}),"\n",(0,o.jsx)(t.p,{children:"you can refine and optimize the descriptions for each tool call function. The LLM chooses its tools based on these descriptions, so it's vital to craft them in a way that aligns with typical user queries."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.em,{children:"If the LLM hallucinates and produces tool calls with non-existent function names or incorrect parameters,"})}),"\n",(0,o.jsx)(t.p,{children:"the agent app should identify this issue and prompt the LLM to create a new response."}),"\n",(0,o.jsx)(t.p,{children:"Tool calling is a fundamental feature in the evolving field of agentic LLM applications. We\u2019re eager to see the innovative ideas you bring forward!"})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>l});var a=n(6540);const o={},i=a.createContext(o);function s(e){const t=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(i.Provider,{value:t},e.children)}}}]);