"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3840],{7192:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"tutorial/llamacpp","title":"Fine-tune an open-source LLM with llama.cpp","description":"You could fine-tune an open-source LLM to","source":"@site/docs/tutorial/llamacpp.md","sourceDirName":"tutorial","slug":"/tutorial/llamacpp","permalink":"/tutorial/llamacpp","draft":false,"unlisted":false,"editUrl":"https://github.com/GaiaNet-AI/docs/edit/main/docs/tutorial/llamacpp.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Agentic translation on Gaia","permalink":"/tutorial/translator-agent"},"next":{"title":"Working with Coinbase AgentKit","permalink":"/tutorial/coinbase"}}');var a=t(4848),o=t(8453);const s={sidebar_position:2},r="Fine-tune an open-source LLM with llama.cpp",l={},c=[{value:"Build the fine-tune utility from llama.cpp",id:"build-the-fine-tune-utility-from-llamacpp",level:2},{value:"Get the base model",id:"get-the-base-model",level:2},{value:"Create a question and answer set for fine-tuning",id:"create-a-question-and-answer-set-for-fine-tuning",level:2},{value:"Finetune!",id:"finetune",level:2},{value:"Merge",id:"merge",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"fine-tune-an-open-source-llm-with-llamacpp",children:"Fine-tune an open-source LLM with llama.cpp"})}),"\n",(0,a.jsx)(n.h1,{id:"fine-tune-llms",children:"Fine-tune LLMs"}),"\n",(0,a.jsx)(n.p,{children:"You could fine-tune an open-source LLM to"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Teach it to follow conversations."}),"\n",(0,a.jsx)(n.li,{children:"Teach it to respect and follow instructions."}),"\n",(0,a.jsx)(n.li,{children:"Make it refuse to answer certain questions."}),"\n",(0,a.jsx)(n.li,{children:'Give it a specific "speaking" style.'}),"\n",(0,a.jsx)(n.li,{children:"Make it response in certain formats (e.g., JSON)."}),"\n",(0,a.jsx)(n.li,{children:"Give it focus on a specific domain area."}),"\n",(0,a.jsx)(n.li,{children:"Teach it certain knowledge."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"To do that, you need to create a set of question and answer pairs to show the model the prompt and the expected response.\nThen, you can use a fine-tuning tool to perform the training and make the model respond the expected answer for each question."}),"\n",(0,a.jsx)(n.h1,{id:"how-to-fine-tune-an-open-source-llm-with-llamacpp",children:"How to fine-tune an open-source LLM with llama.cpp"}),"\n",(0,a.jsxs)(n.p,{children:["The popular llama.cpp tool comes with a ",(0,a.jsx)(n.code,{children:"finetune"})," utility. It works well on CPUs! This fine-tune guide is reproduced with permission from Tony Yuan's ",(0,a.jsx)(n.a,{href:"https://github.com/YuanTony/chemistry-assistant/tree/main/fine-tune-model",children:"Finetune an open-source LLM for the chemistry subject"})," project."]}),"\n",(0,a.jsx)(n.h2,{id:"build-the-fine-tune-utility-from-llamacpp",children:"Build the fine-tune utility from llama.cpp"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"finetune"})," utility in llama.cpp can work with quantized GGUF files on CPUs, and hence dramatically reducing the hardware requirements and expenses for fine-tuning LLMs."]}),"\n",(0,a.jsx)(n.p,{children:"Check out and download the llama.cpp source code."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"git clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n"})}),"\n",(0,a.jsx)(n.p,{children:"Build the llama.cpp binary."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"mkdir build\ncd build\ncmake ..\ncmake --build . --config Release\n"})}),"\n",(0,a.jsx)(n.p,{children:"If you have NVIDIA GPU and CUDA toolkit installed, you should build llama.cpp with CUDA support."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"mkdir build\ncd build\ncmake .. -DLLAMA_CUBLAS=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc\ncmake --build . --config Release\n"})}),"\n",(0,a.jsx)(n.h2,{id:"get-the-base-model",children:"Get the base model"}),"\n",(0,a.jsx)(n.p,{children:"We are going to use Meta's Llama2 chat 13B model as the base model. Note that we are using a Q5 quantized GGUF model file directly to save computing resources. You can use any of the Llama2 compatible GGUF models on Hugging Face."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"cd .. # change to the llama.cpp directory\ncd models/\ncurl -LO https://huggingface.co/gaianet/Llama-2-13B-Chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf\n"})}),"\n",(0,a.jsx)(n.h2,{id:"create-a-question-and-answer-set-for-fine-tuning",children:"Create a question and answer set for fine-tuning"}),"\n",(0,a.jsxs)(n.p,{children:["Next we came up with 1700+ pairs of QAs for the chemistry subject. It is like the following in a ",(0,a.jsx)(n.a,{href:"https://raw.githubusercontent.com/YuanTony/chemistry-assistant/main/fine-tune-model/train.csv",children:"CSV file"}),"."]}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Question"}),(0,a.jsx)(n.th,{children:"Answer"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"What is unique about hydrogen?"}),(0,a.jsx)(n.td,{children:"It's the most abundant element in the universe, making up over 75% of all matter."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"What is the main component of Jupiter?"}),(0,a.jsx)(n.td,{children:"Hydrogen is the main component of Jupiter and the other gas giant planets."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Can hydrogen be used as fuel?"}),(0,a.jsx)(n.td,{children:"Yes, hydrogen is used as rocket fuel. It can also power fuel cells to generate electricity."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"What is mercury's atomic number?"}),(0,a.jsx)(n.td,{children:"The atomic number of mercury is 80"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"What is Mercury?"}),(0,a.jsx)(n.td,{children:"Mercury is a silver colored metal that is liquid at room temperature. It has an atomic number of 80 on the periodic table. It is toxic to humans."})]})]})]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:"We used GPT-4 to help me come up many of these QAs."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Then, we wrote a ",(0,a.jsx)(n.a,{href:"https://raw.githubusercontent.com/YuanTony/chemistry-assistant/main/fine-tune-model/convert.py",children:"Python script"})," to convert each row in the CSV file into a sample QA in the Llama2 chat template format. Notice that each QA pair starts with ",(0,a.jsx)(n.code,{children:"<SFT>"})," as an indicator for the fine-tune program to start a sample. The result ",(0,a.jsx)(n.a,{href:"https://raw.githubusercontent.com/YuanTony/chemistry-assistant/main/fine-tune-model/train.txt",children:"train.txt"})," file can now be used in fine-tuning."]}),"\n",(0,a.jsxs)(n.p,{children:["Put the ",(0,a.jsx)(n.a,{href:"https://raw.githubusercontent.com/YuanTony/chemistry-assistant/main/fine-tune-model/train.txt",children:"train.txt"})," file in the ",(0,a.jsx)(n.code,{children:"llama.cpp/models"})," directory with the GGUF base model."]}),"\n",(0,a.jsx)(n.h2,{id:"finetune",children:"Finetune!"}),"\n",(0,a.jsx)(n.p,{children:"Use the following command to start the fine-tuning process on your CPUs. I am putting it in the background so that it can run continuously now.\nIt could take several days or even a couple of weeks depending on how many CPUs you have."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"nohup ../build/bin/finetune --model-base llama-2-13b-chat.Q5_K_M.gguf --lora-out lora.bin --train-data train.txt --sample-start '<SFT>' --adam-iter 1024 &\n"})}),"\n",(0,a.jsxs)(n.p,{children:["You can check the process every few hours in the ",(0,a.jsx)(n.code,{children:"nohup.out"})," file. It will report the ",(0,a.jsx)(n.code,{children:"loss"})," for each iteration. You can stop the process when the ",(0,a.jsx)(n.code,{children:"loss"})," goes consistently under ",(0,a.jsx)(n.code,{children:"0.1"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note 1"})," If you have multiple CPUs (or CPU cores), you can speed up the fine-tuning process by adding a ",(0,a.jsx)(n.code,{children:"-t"})," parameter to the above command to use more threads. For example, if you have 60 CPU cores, you could do ",(0,a.jsx)(n.code,{children:"-t 60"})," to use all of them."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note 2"})," If your fine-tuning process is interrupted, you can restart it from ",(0,a.jsx)(n.code,{children:"checkpoint-250.gguf"}),". The next file it outputs is ",(0,a.jsx)(n.code,{children:"checkpoint-260.gguf"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"nohup ../build/bin/finetune --model-base llama-2-13b-chat.Q5_K_M.gguf --checkpoint-in checkpoint-250.gguf --lora-out lora.bin --train-data train.txt --sample-start '<SFT>' --adam-iter 1024 &\n"})}),"\n",(0,a.jsx)(n.h2,{id:"merge",children:"Merge"}),"\n",(0,a.jsxs)(n.p,{children:["The fine-tuning process updates several layers of the LLM's neural network. Those updated layers are saved in a file called ",(0,a.jsx)(n.code,{children:"lora.bin"})," and you can now merge them back to the base LLM to create the new fine-tuned LLM."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"../build/bin/export-lora --model-base llama-2-13b-chat.Q5_K_M.gguf --lora lora.bin --model-out chemistry-assistant-13b-q5_k_m.gguf\n"})}),"\n",(0,a.jsx)(n.p,{children:"The result is this file."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"curl -LO https://huggingface.co/juntaoyuan/chemistry-assistant-13b/resolve/main/chemistry-assistant-13b-q5_k_m.gguf\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note 3"})," If you want to use a checkpoint to generate a ",(0,a.jsx)(n.code,{children:"lora.bin"})," file, use the following command. This is needed when you believe the final ",(0,a.jsx)(n.code,{children:"lora.bin"})," is an overfit."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"../build/bin/finetune --model-base llama-2-13b-chat.Q5_K_M.gguf --checkpoint-in checkpoint-250.gguf --only-write-lora --lora-out lora.bin\n"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var i=t(6540);const a={},o=i.createContext(a);function s(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);